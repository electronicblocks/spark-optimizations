{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8b1c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f585f2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/arphaxad/.local/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/arphaxad/.local/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/arphaxad/.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/arphaxad/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7148d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d4ea577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set\n",
    "key_1 = [101]*100\n",
    "key_2 = [201]* 700000\n",
    "key_3 = [301] * 500\n",
    "key_4 = [401] *1000\n",
    "orderID = key_1 + key_2 +key_3 +key_4\n",
    "random.shuffle(orderID)\n",
    "\n",
    "qty_1=    list(np.random.randint(low = 1,high = 100,size = len(key_1)))\n",
    "qty_2 =   list(np.random.randint(low = 1,high = 200,size = len(key_2))) \n",
    "qty_3 = list(np.random.randint(low = 1,high = 1000,size = len(key_3)))\n",
    "qty_4 = list(np.random.randint(low = 1,high = 50,size = len(key_4)))\n",
    "qty = qty_1+qty_2+qty_3+qty_4\n",
    "\n",
    "sales_1 = list(np.random.randint(low  = 10,high = 100,size = len(key_1)))\n",
    "sales_2 = list(np.random.randint(low = 50 ,high = 3400 ,size = len(key_2)))\n",
    "sales_3 = list(np.random.randint(low = 12 ,high =2000 ,size = len(key_3)))\n",
    "sales_4 = list(np.random.randint(low = 40 ,high = 1000,size = len(key_4)))\n",
    "sales = sales_1 + sales_2+sales_3 +sales_4\n",
    "discount = list(np.random.randint(low  =0,high = 2,size = len(orderID)))\n",
    "data1 = list(zip(orderID,qty,sales,discount))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6fab54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create pandas dataframe\n",
    "data_skew = pd.DataFrame(data1,columns = ['orderID','qty','sales','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb442cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small data \n",
    "\n",
    "data2 = [[1,101,'pencil',4.99],\n",
    "         [2,101,'book',9.5],\n",
    "         [3,101,'scissors',14],\n",
    "         [4,301,'glue',7],\n",
    "         [5,201,'marker',8.49],\n",
    "         [6,301,'label',2],\n",
    "         [7,201,'calculator',3.99],\n",
    "         [8,501,'eraser',1.55]\n",
    "    \n",
    "    \n",
    "]\n",
    "data_small = pd.DataFrame(data2,columns = ['productID','orderID','product','price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b9acbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pandas to spark dataframe \n",
    "\n",
    "# spark.conf.set('spark.sql.execution.arrow.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac7cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get('spark.sql.execution.arrow.enabled'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cc527dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 20:16:46 WARN Utils: Your hostname, arphaxad-HP-Notebook resolves to a loopback address: 127.0.1.1; using 192.168.1.4 instead (on interface wlp3s0)\n",
      "24/04/12 20:16:46 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/12 20:16:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>skew_df</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74f5d394ab30>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"skew_df\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73660927",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get('spark.sql.execution.arrow.pyspark.enabled'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3afa2175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 20:22:02 WARN SQLConf: The SQL config 'spark.sql.execution.arrow.enabled' has been deprecated in Spark v3.0 and may be removed in the future. Use 'spark.sql.execution.arrow.pyspark.enabled' instead of it.\n"
     ]
    }
   ],
   "source": [
    "# convert pandas to spark dataframe \n",
    "\n",
    "spark.conf.set('spark.sql.execution.arrow.enabled','true')\n",
    "spark.conf.set('spark.sql.execution.arrow.pyspark.enabled','true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f203dfec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arphaxad/.local/lib/python3.10/site-packages/pyspark/sql/pandas/conversion.py:351: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  PyArrow >= 4.0.0 must be installed; however, it was not found.\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- orderID: long (nullable = true)\n",
      " |-- qty: long (nullable = true)\n",
      " |-- sales: long (nullable = true)\n",
      " |-- price: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_skew = spark.createDataFrame(data_skew)\n",
    "df_skew.printSchema()\n",
    "df_skew.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359144ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- productID: long (nullable = true)\n",
      " |-- orderID: long (nullable = true)\n",
      " |-- product: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small  = spark.createDataFrame(data_small)\n",
    "df_small.printSchema()\n",
    "df_small.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2c1dbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_skew.show()\n",
    "df_skew = df_skew.repartition(8)\n",
    "df_small = df_small.repartition(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6e9ff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")..............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b40095a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_small.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a600478",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_skew.join(df_small,df_skew.orderID==df_small.orderID,how = \"inner\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c852c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "011d65b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'joined_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2371287/2177738839.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjoined_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'joined_df' is not defined"
     ]
    }
   ],
   "source": [
    "joined_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined_df.(4)\n",
    "joined_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426576d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spark.conf.get(\"spark.sql.shuffle.partitions\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec37aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df.rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d434501",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e01f913f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 20:30:23 WARN TaskSetManager: Stage 47 contains a task of very large size (2053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 50:==================================================>       (7 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+---------+---------+\n",
      "|         avg_sales|          std_dev|min sales|max sales|\n",
      "+------------------+-----------------+---------+---------+\n",
      "|1721.5427024905446|967.4304609758726|       10|     3399|\n",
      "+------------------+-----------------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "groups = df_skew.join(df_small,df_skew.orderID == df_small.orderID, how=\"inner\")\n",
    "summary = groups.select(mean(groups.sales).alias(\"avg_sales\"),\n",
    "                       stddev(groups.sales).alias(\"std_dev\"),\n",
    "                        min(groups.sales).alias(\"min sales\"),\n",
    "                        max(groups.sales).alias(\"max sales\")\n",
    "                       )\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_skew_salt = df_skew.withColumn(\"salt\",round(rand()*2))\n",
    "df_small_salt = df_small.withColumn(\"salt\",round(rand()*2))\n",
    "                      \n",
    "                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "15ec37f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c6b77e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/13 00:17:25 WARN TaskSetManager: Stage 63 contains a task of very large size (2053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+---------+---------+\n",
      "|         avg_sales|          std_dev|min sales|max sales|\n",
      "+------------------+-----------------+---------+---------+\n",
      "|1721.5427024905446|967.4304609758591|       10|     3399|\n",
      "+------------------+-----------------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 71:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#salting\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\",8)\n",
    "from pyspark.sql.functions import *\n",
    "df_skew_salt = df_skew.withColumn(\"salt\",round(rand()*7))\n",
    "df_small_salt = df_small.withColumn(\"salt\",round(rand()*27))\n",
    "                      \n",
    "df_skew_salt = df_skew_salt.repartition(8,\"salt\")\n",
    "df_small_salt = df_small_salt.repartition(8,\"salt\")\n",
    "groups = df_skew.join(df_small,df_skew.orderID == df_small.orderID, how=\"inner\")\n",
    "summary = groups.select(mean(groups.sales).alias(\"avg_sales\"),\n",
    "                       stddev(groups.sales).alias(\"std_dev\"),\n",
    "                        min(groups.sales).alias(\"min sales\"),\n",
    "                        max(groups.sales).alias(\"max sales\")\n",
    "                       )\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0f12a8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/13 00:18:07 WARN TaskSetManager: Stage 76 contains a task of very large size (2053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de2659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#repsrtition\n",
    "df_skew_salt = df_skew_salt.repartition(3,\"salt\")\n",
    "df_small_salt = df_small_salt.repartition(3,\"salt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874f283",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_skew_salt.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d511c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "groups = df_skew_salt.join(df_small_salt,df_skew_salt.orderID == df_small_salt.orderID, how=\"inner\")\n",
    "summary = groups.select(mean(groups.sales).alias(\"avg_sales\"),\n",
    "                       stddev(groups.sales).alias(\"std_dev\"),\n",
    "                        min(groups.sales).alias(\"min sales\"),\n",
    "                        max(groups.sales).alias(\"max sales\")\n",
    "                       )\n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30414174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/12 23:53:28 WARN TaskSetManager: Stage 60 contains a task of very large size (2053 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 60:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----------------+---------+---------+\n",
      "|         avf sales|          std dev|min sales|max sales|\n",
      "+------------------+-----------------+---------+---------+\n",
      "|1721.5427024905446|967.4304609758585|       10|     3399|\n",
      "+------------------+-----------------+---------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "groups_brd = df_skew.join(broadcast(df_small),df_skew.orderID == df_small.orderID,how = \"inner\")\n",
    "summ = groups_brd.select(mean(groups_brd.sales).alias(\"avf sales\"),\n",
    "                        stddev(groups_brd.sales).alias(\"std dev\"),\n",
    "                         min(groups_brd.sales).alias(\"min sales\"),\n",
    "                         max(groups_brd.sales).alias(\"max sales\"))\n",
    "summ.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "993bee5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728b\n"
     ]
    }
   ],
   "source": [
    "print((spark.conf.get(\"spark.sql.files.maxPartitionBytes\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "99cb61fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "134217728/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "12de2e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\",\"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c27de6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
